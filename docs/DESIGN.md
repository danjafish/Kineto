# Kineto Design & Evaluation

## 1. High-Level Overview

Kineto is an end-to-end system that takes an OpenAPI specification and produces a fully functional FastAPI backend, complete with automated tests and self-correcting mechanisms. It consists of three core capabilities:

1. **Structured Code Generation**

   * The OpenAPI spec is parsed into discrete pieces: data models, route handlers (grouped by tags), the application entry point, dependency declarations, and container settings (requirements & Dockerfile).
   * Each piece is generated by issuing a focused prompt to the LLM, ensuring clarity of intent and minimizing prompt bloat.

2. **Automated Testing & Refinement**

   * A pytest-based contract test suite is generated alongside the code, covering CRUD operations, status codes, response shapes, and authorization rules.
   * The system runs the test suite against the freshly generated app, captures failures, and invokes the LLM to patch the relevant router files.
   * Up to two iterations of test→refine loops are performed to achieve a passing build.

3. **Transparent Metadata & Logs**

   * Every prompt sent to the LLM and its corresponding response—both at generation and refinement stages—is recorded in `metadata.json` and `refiner_log.json`.
   * Lint results, error logs, and timestamps are preserved for auditing and to guide downstream agents or human operators.

### Key Files & Their Roles

| File                  | Description                                                                                                              |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `generator.py`        | Main orchestrator: loads spec, issues code-generation prompts, runs lint-based self-refinements, writes `metadata.json`. |
| `agent/llm_client.py` | Centralizes OpenAI API interactions, handling retries, logging, and model configuration.                                 |
| `agent/prompts.py`    | Houses all prompt templates for initial code generation and refinements.                                                 |
| `agent/evaluator.py`  | Provides linting checks (flake8) against generated code and reports errors.                                              |
| `entrypoint.sh`       | Docker entrypoint: runs generator, starts the server, conducts test→refine loops, and launches the final app.            |
| `refiner.py`          | On test failures, reads error logs and spec, issues router-wide refinement prompts, writes `refiner_log.json`.           |
| `app/`                | Generated application code: `models.py`, `routes/*.py`, and `main.py`.                                                   |
| `tests/test_api.py`   | Autogenerated pytest suite enforcing spec-defined behaviors.                                                             |

## 2. Pipeline Walkthrough

Below is a step-by-step account of what happens inside the Docker container when you run:

```bash
docker run --rm \
  -v /path/to/spec:/spec \
  -v /path/to/out:/app \
  -p 8000:8000 \
  kineto-app /spec/openapi.yaml /app
```

1. **Generation Phase** (`[GEN]`)

   * **Command**: `python generator.py /spec/openapi.yaml -o /app`
   * **Actions**:

     * Parse the OpenAPI YAML/JSON.
     * Build a list of targets: data models, router modules per tag, main entry-point, requirements.txt, Dockerfile.
     * For each target, fill in the corresponding prompt template with the relevant spec snippet.
     * Call the LLM to generate raw code, write to disk.
     * Run flake8 via `agent/evaluator.py`; if errors, loop up to two lint-refinement prompts.
     * Output `metadata.json` capturing each prompt and response along with any lint corrections.

2. **Server Startup** (`[RUN]`)

   * **Command**: `uvicorn app.main:app --host 0.0.0.0 --port 8000 &`
   * Launch the FastAPI service in the background, forwarding signals.

3. **Test & Refinement Loop**

   * Iterations: up to 2
   * **Testing** (`[TEST]`):

     * Temporarily disable `set -e`.
     * Run `pytest tests/test_api.py -v | tee test_results.log`.
     * Capture exit status in `TEST_RC`; re-enable `set -e`.
     * If all tests pass, break the loop.
   * **Refinement** (`[REFINER]`):

     * When `TEST_RC != 0` and iteration < 2:

       * Call `python refiner.py /spec/openapi.yaml /app test_results.log`.
       * `refiner.py` loads the spec and tests log, then for each router file issues a single router-wide refinement prompt with code, spec JSON, and error log.
       * Overwrites route modules where the LLM returns changes.
       * Appends entries to `refiner_log.json` (filename, prompt, response, errors, timestamp).
     * Repeat testing.

4. **Final Serve** (`[RUN]`)

   * If any iteration yields a passing test suite, invoke `exec uvicorn…` to replace the shell and keep the server running.
   * If both iterations fail, exit with a non-zero code.

## 3. Evaluation Approach (Generation-Centric)

Our evaluation focuses on **how well the generator produces correct and maintainable code**, rather than on the runtime application itself.

### 3.1 Implemented Checks

* **Contract Test Pass Rate**: measure the fraction of tests passing without refinement (iteration 1) vs. after refinement (iteration 2).
* **Lint Compliance**: record the number of flake8 errors initially and after each self-refinement.
* **Metadata Completeness**: ensure every generation and refinement prompt/response pair is logged in `metadata.json` and `refiner_log.json`.

### 3.2 Gaps & Planned Extensions

Although the pipeline automates lint-based and test-driven refinement, several evaluation dimensions are not yet implemented but could significantly strengthen confidence in the generator:

1. **Spec–Code Coverage Analysis**

   * Automatically verify that every path, parameter, and schema defined in the OpenAPI spec has a corresponding route or model class.
   * Use semantic matching (e.g. embeddings) to detect unhandled or partially handled spec elements.

2. **Prompt Template Benchmarking**

   * Track metrics on prompt token counts, LLM latency, and response quality.
   * A/B test alternative prompt formulations to discover more efficient or accurate templates.

3. **Golden-Reference Comparison**

   * Maintain a set of hand-crafted “golden” outputs for representative specs.
   * Diff generated code against these references to detect regressions or unintended deviations.

4. **Automated Model Selection**

   * Dynamically choose between GPT‐3.5, GPT‐4, or domain‐specific fine‐tuned models based on spec size or complexity metrics.

5. **Incremental Generation Modes**

   * Allow “partial regeneration” of only changed spec sections, avoiding full rebuilds.
   * Cache generation outputs for unchanged snippet blocks.

## 4. Future Enhancements (Generator-Focused)

To improve the system’s ability to serve diverse user needs and boost robustness, we plan to:

* **Dynamic Prompt Scheduling**: Introduce a planning agent that splits large specs into smaller batches, orchestrates prompt order (models first, then routes, etc.), and parallelizes LLM calls.
* **Test-Driven Generation**: Generate the pytest suite first, then generate code by instructing the LLM to satisfy each test in sequence, ensuring 100% pass rate.
* **Adaptive Refinement Strategies**: Analyze error logs to decide between a router-wide patch vs. endpoint-specific patch; escalate to schema refactoring if needed.
* **Multi-Agent Collaboration**: Assign separate AI agents as “planner,” “coder,” “tester,” and “optimizer” coordinated by a workflow engine for better separation of concerns.
* **Language & Framework Agnosticism**: Abstract prompt templates and file-scaffold logic so that users can target Express.js, Spring Boot, or other stacks by swapping in new prompt sets.
* **Continuous Prompt Learning**: Collect anonymized metadata on prompt success rates, then periodically fine-tune or retrain smaller, specialized models on that data to reduce reliance on API latency and cost.

