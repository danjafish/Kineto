# Kineto Design & Evaluation

## 1. High-Level Overview

Kineto is an end-to-end system that takes an OpenAPI specification and produces a fully functional FastAPI backend, complete with automated tests and self-correcting mechanisms. It consists of three core capabilities:

1. **Structured Code Generation**

   * The OpenAPI spec is parsed into clear, focused fragments:

     * **Data Models**: Pydantic classes for each schema
     * **Route Handlers**: grouped by tags, one router module per tag
     * **Application Entrypoint**: `main.py`
     * **Dependencies & Environment**: requirements.txt, Dockerfile
   * Each fragment is generated by a concise LLM prompt tailored to its purpose—this avoids “prompt bloat,” i.e. sending the entire spec every time and overwhelming the model.

2. **Automated Testing & Refinement**

   * A pytest-based contract test suite is generated alongside the code. It systematically exercises all **CRUD** operations—Create, Read, Update, Delete—validates status codes (200, 201, 400, 404, 401, etc.), response payload shapes, and authorization rules if specified.
   * After initial code generation, the system launches the service, runs the tests, captures failures, and invokes the LLM to patch the relevant router modules via a single **REFINE\_ROUTER** prompt.
   * It repeats test→refine cycles up to a configurable limit (default: two iterations) to balance turnaround time and cost; additional iterations can be enabled in an **Optional Local Mode** for deeper self-healing experiments.

3. **Transparent Metadata & Logs**

   * Every prompt and response at both generation and refinement stages is recorded in `metadata.json` and `refiner_log.json`, including the exact spec snippet, error logs, and timestamps.
   * Linting outputs, test results, and revision counts are preserved to give full visibility into why and how code evolved.

### Key Files & Their Roles

| File                  | Description                                                                                                              |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| `generator.py`        | Main orchestrator: loads spec, issues code-generation prompts, runs lint-based self-refinements, writes `metadata.json`. |
| `agent/llm_client.py` | Centralizes OpenAI API interactions, handling retries, logging, and model configuration.                                 |
| `agent/prompts.py`    | Defines all prompt templates for initial code generation, lint fixes, and router refinement.                             |
| `agent/evaluator.py`  | Executes flake8 on generated output and returns lint errors for refinement.                                              |
| `entrypoint.sh`       | Docker entrypoint: generate → serve → test → refine loop → final serve.                                                  |
| `refiner.py`          | Processes `test_results.log` and spec, issues a router-wide refine prompt per module, writes `refiner_log.json`.         |
| `app/`                | Generated application code: `models.py`, `routes/*.py`, `main.py`.                                                       |
| `tests/test_api.py`   | Autogenerated pytest suite enforcing every spec-defined endpoint and error case.                                         |

## 2. Pipeline Walkthrough

This section explains every step inside the Docker container when running:

```bash
docker run --rm \
  -v /path/to/spec:/spec \
  -v /path/to/out:/app \
  -p 8000:8000 \
  kineto-app /spec/openapi.yaml /app
```

1. **Generation Phase** (`[GEN]`)

   * **Command**: `python generator.py /spec/openapi.yaml -o /app`
   * **Actions**:
     • Parse the OpenAPI spec into schemas, paths, servers, and dependencies.
     • Plan target outputs: models, routers (by tag), main, requirements, Dockerfile.
     • Issue a focused LLM prompt for each target, supplying just the relevant snippet.
     • Write raw code, then run flake8 via `agent/evaluator.py`.
     • For any lint errors, send up to two successive lint-refine prompts.
     • Produce `metadata.json` logging all prompts, responses, and lint corrections.

2. **Server Startup** (`[RUN]`)

   * **Command**: `uvicorn app.main:app --host 0.0.0.0 --port 8000 &`
   * Starts the FastAPI service in background; signals (TERM/INT) are trapped and forwarded.
   * If the server fails to start, the entrypoint captures the failure log and proceeds to a refinement attempt rather than simply exiting.

3. **Test & Refinement Loop** (up to N iterations)

   * **Testing** (`[TEST]`):

     ```bash
     set +e
     pytest tests/test_api.py -v | tee test_results.log
     TEST_RC=${PIPESTATUS[0]}
     set -e
     ```

     • Runs the contract tests and logs output.
     • Captures the exit code without aborting the whole script.
   * **Refinement** (`[REFINER]`): only if `TEST_RC != 0` and iterations < limit.

     ```bash
     python refiner.py /spec/openapi.yaml /app test_results.log
     ```

     • Reads the full spec and `test_results.log`.
     • For each file in `app/routes/`, sends a single router-wide **REFINE\_ROUTER** prompt with code, spec JSON, and error log.
     • Overwrites modules where the LLM returns changes.
     • Writes `refiner_log.json` entries (filename, prompt, response, errors, timestamp).
   * Repeat testing and refinement until either tests pass or iteration limit is reached.

4. **Final Serve** (`[RUN]`)

   * If tests pass within the allowed iterations, `exec uvicorn…` replaces the shell and keeps the server running in the foreground.
   * If tests still fail, the container exits with a non-zero code and preserves logs for post-mortem.

## 3. Evaluation Approach (Generation-Centric)

We measure the efficacy and efficiency of the **code-generation process** rather than focusing on deployment or runtime metrics.

### 3.1 Implemented Metrics

* **First-Pass Success Rate**: percentage of specs whose entire test suite passes without any refinement.
* **Average Refinements**: mean number of test→refine iterations required.
* **Lint Pass Rate**: proportion of files needing zero vs. one vs. two lint-refine cycles.
* **Prompt & Model Efficiency**: count of LLM calls, tokens consumed per file, and round-trip latency.

### 3.2 Planned Extensions

1. **Spec–Code Coverage Analysis**

   * Automatically confirm every path, parameter, and schema is represented in the generated code.
   * Use semantic embeddings to identify unaddressed spec elements.

2. **Prompt Template Benchmarking**

   * Track token usage, error rates, and code quality across multiple template variants.
   * A/B test different prompt structures against the same spec and compare downstream metrics.

3. **Golden-Reference Diffing**

   * While we haven’t yet maintained hand-crafted outputs, we envision a suite of “golden” backends for key specs.
   * Automated diffs would highlight unintended changes or regressions.

4. **Adaptive Model Selection**

   * Dynamically route simple tasks (models, requirements) to smaller models, and complex code-generation (routers, main) to larger GPT-4 or fine-tuned variants.

5. **Incremental & Cached Generation**

   * When specs change, regenerate only affected fragments; reuse previous outputs for unchanged parts.

## 4. Future Enhancements (Generator-Focused)

To better serve varied user needs and increase reliability:

* **Dynamic Prompt Planning**: implement a “planner” agent that splits large specs into manageable chunks, orders generation tasks, and parallelizes LLM calls when safe.
* **Test-First Generation**: reverse the workflow by generating tests from the spec first, then drive code generation to satisfy each test—ensuring 100% coverage by construction.
* **Selective Refinement Strategies**: inspect error logs to choose between router-wide patches vs. single-handler edits, or to trigger schema-model regeneration if mismatches arise.
* **Multi-Agent Collaboration**: assign specialized roles—planner, coder, tester, optimizer—to separate agents coordinated by a workflow engine.
* **Framework Agnosticism**: abstract prompt and file-scaffold templates so that replacing FastAPI with Express.js, Spring Boot, or other stacks is a matter of swapping prompt sets.
* **Continuous Prompt Learning**: collect anonymized feedback on prompt success rates and periodic fine-tuning to reduce reliance on expensive API calls.

